{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ShopEasy\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_conn = psycopg2.connect(\n",
    "    dbname=\"shopeasy_dw\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123\",\n",
    "    host=\"localhost\"\n",
    ")\n",
    "pg_cursor = pg_conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_cursor.execute(\"SELECT * FROM dim_users\")\n",
    "users_data = pg_cursor.fetchall()\n",
    "pg_cursor.execute(\"SELECT * FROM dim_products\")\n",
    "products_data = pg_cursor.fetchall()\n",
    "pg_cursor.execute(\"SELECT * FROM dim_categories\")\n",
    "categories_data = pg_cursor.fetchall()\n",
    "pg_cursor.execute(\"SELECT * FROM dim_dates\")\n",
    "dates_data = pg_cursor.fetchall()\n",
    "pg_cursor.execute(\"SELECT * FROM fact_sales\")\n",
    "sales_data = pg_cursor.fetchall()\n",
    "pg_conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_columns = [\"user_id\", \"name\", \"email\", \"address\"]\n",
    "products_columns = [\"product_id\", \"product_name\", \"category_id\", \"price\"]\n",
    "categories_columns = [\"category_id\", \"category\"]\n",
    "dates_columns = [\"date_id\", \"order_date\", \"day_of_week\", \"day_type\"]\n",
    "sales_columns = [\"sale_id\", \"user_id\", \"product_id\", \"category_id\", \"date_id\", \"quantity\", \"total_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.createDataFrame(users_data, users_columns)\n",
    "products_df = spark.createDataFrame(products_data, products_columns)\n",
    "categories_df = spark.createDataFrame(categories_data, categories_columns)\n",
    "dates_df = spark.createDataFrame(dates_data, dates_columns)\n",
    "sales_df = spark.createDataFrame(sales_data, sales_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"file:///C:/Users/User/Desktop/project2/bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/users\")\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/products\")\n",
    "categories_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/categories\")\n",
    "dates_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/dates\")\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+--------------------+\n",
      "|user_id|               name|               email|             address|\n",
      "+-------+-------------------+--------------------+--------------------+\n",
      "|    126|    Jeremy Castillo|sandrareynolds@ex...|61052 Mckinney Gl...|\n",
      "|    127|       Calvin Silva| david75@example.org|Unit 0447 Box 813...|\n",
      "|    128|         Jamie Soto|  cody26@example.org|3644 Trujillo Roa...|\n",
      "|    129|     Vicki Martinez| vmoreno@example.org|741 James Run Apt...|\n",
      "|    130|     Patricia Reyes|debbie54@example.net|1700 Nathan Hollo...|\n",
      "|    131|     Lacey Richards|ssullivan@example...|5645 Thompson Roa...|\n",
      "|    132|Jeffrey Crawford MD|goodwincrystal@ex...|8142 Erik Motorwa...|\n",
      "|    133|        Linda Cline|herringalan@examp...|1937 Lauren Plaza...|\n",
      "|    134|   Melissa Anderson|  mark38@example.org|PSC 2433, Box 677...|\n",
      "|    135|    Sharon Davidson|melanie73@example...|0837 Sarah Island...|\n",
      "|    136|    Margaret Taylor|tracyfreeman@exam...|575 Carr Turnpike...|\n",
      "|    137|        Ricky Reyes| cathy70@example.com|865 Fisher Spurs ...|\n",
      "|    138|    Gregory Griffin|amybarry@example.com|0324 Melissa Squa...|\n",
      "|    139|       Darren Smith|   gkent@example.net|413 Ashley Garden...|\n",
      "|    140|       Brian Tucker| shawn87@example.com|55433 Reeves Plai...|\n",
      "|    141|        Jaime Lopez|shaunlee@example.org|38172 Brown Brook...|\n",
      "|    142|    Jessica Johnson|jamesreed@example...|6219 Tran Village...|\n",
      "|    143|      Anthony Smith| ewarner@example.net|6676 Clayton Loop...|\n",
      "|    144|   Stacy Mclaughlin|rebeccamarquez@ex...|43095 Porter Spur...|\n",
      "|    145|     Sheryl Stanley|  qbrown@example.org|238 Brown Run Sui...|\n",
      "+-------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users = spark.read.format(\"delta\").load(f\"{bronze_path}/users\")\n",
    "df_users.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "silver_path = \"C:/Users/User/Desktop/project2/silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_silver = spark.read.format(\"delta\").load(f\"{bronze_path}/users\").dropDuplicates([\"user_id\"])\n",
    "df_products_silver = spark.read.format(\"delta\").load(f\"{bronze_path}/products\").dropDuplicates([\"product_id\"])\n",
    "df_categories_silver = spark.read.format(\"delta\").load(f\"{bronze_path}/categories\").dropDuplicates([\"category_id\"])\n",
    "df_dates_silver = spark.read.format(\"delta\").load(f\"{bronze_path}/dates\").dropDuplicates([\"date_id\"])\n",
    "df_sales_silver = spark.read.format(\"delta\").load(f\"{bronze_path}/sales\").dropDuplicates([\"sale_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_silver = df_products_silver.join(df_categories_silver, \"category_id\", \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_silver = df_sales_silver \\\n",
    "    .join(df_users_silver, \"user_id\", \"inner\") \\\n",
    "    .join(df_products_silver, \"product_id\", \"inner\") \\\n",
    "    .join(df_dates_silver, \"date_id\", \"inner\") \\\n",
    "    .select(\"sale_id\", \"user_id\", \"name\", \"email\", \"product_name\", \"category\", \n",
    "            \"quantity\", \"total_price\", \"order_date\")\n",
    "df_users_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}/users\")\n",
    "df_products_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}/products\")\n",
    "df_sales_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}/sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+--------------------+--------------+--------------------+--------+--------------------+----------+\n",
      "|sale_id|user_id|              name|               email|  product_name|            category|quantity|         total_price|order_date|\n",
      "+-------+-------+------------------+--------------------+--------------+--------------------+--------+--------------------+----------+\n",
      "|   3091|    887| Christopher Young|rrobbins@example.net|     Crime Max|            Clothing|       3|125.9000000000000...|2024-04-18|\n",
      "|   3506|    896|     Jordan Turner|gonzaleslisa@exam...|       Any Max|              makeup|       1|360.9900000000000...|2024-05-25|\n",
      "|   3764|    839|       Erik Bailey|sierra80@example.com|    Relate Max|    Sports & Fitness|       1|36.44000000000000...|2024-06-02|\n",
      "|   4590|     38|      Hannah Davis| aaron64@example.org|       Set Pro|      Grocery & Food|       4|187.6900000000000...|2024-08-21|\n",
      "|   4823|    597|     Alan Marshall| karen24@example.com|          My X|      Grocery & Food|       5|316.4000000000000...|2024-08-21|\n",
      "|   4894|    672|     Sharon Spence|omendoza@example.net|       Own Pro|      Grocery & Food|       3|126.6500000000000...|2024-10-23|\n",
      "|   3106|    775|     Rachel Howard|samanthaphillips@...|   Control Pro|               Books|       1|211.8000000000000...|2025-01-19|\n",
      "|   3199|    114|    Steven Goodwin|stephen86@example...|       Few Max|               Books|       3|450.0500000000000...|2024-06-18|\n",
      "|   3280|    927|      John Lindsey|lorisalazar@examp...|  Service Plus|    Sports & Fitness|       2|425.3700000000000...|2025-01-05|\n",
      "|   3391|    441|      Noah Collins|kristy12@example.net|  Official Pro|      Grocery & Food|       5|121.3100000000000...|2024-06-26|\n",
      "|   3800|    791|     Jose Martinez|  hhayes@example.net| Response Plus|  Books & Stationery|       3|23.46000000000000...|2024-10-29|\n",
      "|   3806|    319|     Brian Sanchez|curtisramos@examp...| Election Plus|               Books|       5|144.2800000000000...|2024-09-11|\n",
      "|   4126|    228|      Melissa Cole|  iarias@example.net|Government Max|Beauty & Personal...|       5|352.7300000000000...|2025-01-15|\n",
      "|   4371|    326|   Jeffrey Morales|alexis47@example.net|      Ball Pro|                Toys|       3|93.22000000000000...|2024-08-29|\n",
      "|   3155|    560|        Isaac Kidd|zshepard@example.org|      One Plus|  Books & Stationery|       4|211.4400000000000...|2024-06-19|\n",
      "|   3327|    421|     Melissa Mckee|perkinsandrea@exa...|     Young Max|         Electronics|       5|388.6500000000000...|2024-07-30|\n",
      "|   3704|     40|Catherine Davidson|seanthompson@exam...|    Language X|      Home & Kitchen|       3|306.0700000000000...|2024-06-02|\n",
      "|   3741|    858|    Kristie Miller|thomas20@example.net|     Scene Pro|      Home & Kitchen|       3|237.5600000000000...|2024-05-07|\n",
      "|   3845|    503|     Xavier Chavez|hsanders@example.org|     Cover Max|         Electronics|       1|298.7800000000000...|2024-11-24|\n",
      "|   3937|    136|   Margaret Taylor|tracyfreeman@exam...|    Create Max|  Books & Stationery|       3|23.23000000000000...|2024-08-29|\n",
      "+-------+-------+------------------+--------------------+--------------+--------------------+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users = spark.read.format(\"delta\").load(f\"{silver_path}/sales\")\n",
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "gold_path = \"C:/Users/User/Desktop/project2/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sales_silver = spark.read.format(\"delta\").load(f\"{silver_path}/sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_summary = df_sales_silver \\\n",
    "    .groupBy(\"product_name\", \"category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_price\").alias(\"total_revenue\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.countDistinct(\"user_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "df_sales_summary.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/sales_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_customers = df_sales_silver \\\n",
    "    .groupBy(\"user_id\", \"name\", \"email\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_price\").alias(\"total_spent\"),\n",
    "        F.count(\"sale_id\").alias(\"total_purchases\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"total_spent\"))\n",
    "df_top_customers.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/top_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------+----------------+\n",
      "|  product_name|            category|       total_revenue|total_quantity|unique_customers|\n",
      "+--------------+--------------------+--------------------+--------------+----------------+\n",
      "|   Thought Pro|   Health & Wellness|4701.320000000000...|            33|              15|\n",
      "|        Draw X|                Toys|2230.240000000000...|            26|               7|\n",
      "|   Station Max|      Grocery & Food|1309.890000000000...|            17|               6|\n",
      "|Individual Max|    Sports & Fitness|2220.940000000000...|            29|               8|\n",
      "|Population Pro|         Electronics|3300.410000000000...|            30|              11|\n",
      "|   Billion Pro|      Home & Kitchen|1445.860000000000...|            26|               8|\n",
      "|      Want Pro|               Books|2228.800000000000...|            34|              10|\n",
      "|     Again Pro|   Health & Wellness|1486.820000000000...|            21|               8|\n",
      "|   Manager Max|         Electronics|1739.350000000000...|            34|               9|\n",
      "|      Player X|      Home & Kitchen|2181.960000000000...|            32|              10|\n",
      "| Political Pro|            Clothing|3279.460000000000...|            36|              12|\n",
      "|         Cut X|         Electronics|2301.610000000000...|            27|               7|\n",
      "|     Fire Plus|  Books & Stationery|2720.640000000000...|            28|               9|\n",
      "|        With X|   Health & Wellness|2742.220000000000...|            38|              12|\n",
      "|      Once Pro|               Books|4048.990000000000...|            42|              15|\n",
      "|     Upon Plus|   Health & Wellness|2804.160000000000...|            31|              12|\n",
      "|     Fund Plus|         Electronics|5316.910000000000...|            42|              19|\n",
      "|         Top X|                Toys|2312.430000000000...|            33|              11|\n",
      "|       Its Max|Beauty & Personal...|2298.530000000000...|            43|              13|\n",
      "|      Girl Pro|            Clothing|2593.190000000000...|            26|              10|\n",
      "+--------------+--------------------+--------------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users = spark.read.format(\"delta\").load(f\"{gold_path}/sales_summary\")\n",
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Read the Silver Users table\n",
    "users_silver_path = f\"{silver_path}/users\"\n",
    "delta_users = DeltaTable.forPath(spark, users_silver_path)\n",
    "\n",
    "# New data (assume it's coming from the Bronze layer)\n",
    "df_new_users = spark.read.format(\"delta\").load(f\"{bronze_path}/users\")\n",
    "\n",
    "# Perform MERGE (Upsert)\n",
    "delta_users.alias(\"target\").merge(\n",
    "    df_new_users.alias(\"source\"),\n",
    "    \"target.user_id = source.user_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"name\": \"source.name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"address\": \"source.address\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"user_id\": \"source.user_id\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"address\": \"source.address\"\n",
    "}).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp             |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                 |userMetadata|engineInfo                         |\n",
      "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-03-30 17:45:35.66|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 1, numOutputRows -> 10000, numOutputBytes -> 242123}|null        |Apache-Spark/3.4.4 Delta-Lake/2.4.0|\n",
      "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the Delta table\n",
    "sales_silver_table = DeltaTable.forPath(spark, f\"{silver_path}/sales\")\n",
    "\n",
    "# View history (returns a DataFrame with version information)\n",
    "df_history = sales_silver_table.history()  # Default: last 10 versions\n",
    "df_history.show(truncate=False)  # Show full details\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
